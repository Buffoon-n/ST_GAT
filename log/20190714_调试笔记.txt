去掉了一层64-64 ，似乎效果并不好
dropout设置为0.2
残差设置为true
学习率衰减设置为15,25,35（观察原始训练过程发现每次学习率衰减时都会带来巨大的性能提升）

和原文不同的地方：
层数：原文中说的是9层，3（64）+3（128）+3（256），但代码中写的是10层（算上输入层）
dropout:原文中写的dropout是0.5，但是代码中是0
res结构：原文中写的是在每层GCN后面添加了res结构，但是在代码中没有使用res结构

学习率：
观察原始训练过程发现，在每个学习率下降的epoch都会有明显的loss下降，学习率下降带来的效果提升是非常大的
由于添加了GAT层，可以将每次学习率下降的位置适当延后